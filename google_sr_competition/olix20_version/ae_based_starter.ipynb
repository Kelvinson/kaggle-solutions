{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:29.196238Z",
     "start_time": "2017-11-17T09:03:28.644004Z"
    },
    "_cell_guid": "679e0d3e-646d-4e96-9eb0-b362d8c6e51f",
    "_uuid": "0d05e5ce89af3e25d1c1fb244d021a1cfa1a058c"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import array \n",
    "\n",
    "from pydub import AudioSegment\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Conv2D, Flatten, MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPool2D, concatenate, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Input, GRU, RepeatVector, BatchNormalization, TimeDistributed, Conv1D\n",
    "from keras import backend as K\n",
    "from keras.layers import  Conv2D, MaxPooling2D, UpSampling2D, Lambda, Reshape\n",
    "\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import numba\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:29.210749Z",
     "start_time": "2017-11-17T09:03:29.19832Z"
    },
    "_cell_guid": "8ab00801-08b9-44d3-a063-32e82dbf8f58",
    "_uuid": "53c19941676690454dd4b91109976b6c59cb7a40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSSIBLE_LABELS = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "id2name = {i: name for i, name in enumerate(POSSIBLE_LABELS)}\n",
    "name2id = {name: i for i, name in id2name.items()}\n",
    "len(id2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 200 ms, sys: 52 ms, total: 252 ms\n",
      "Wall time: 248 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# train_df = pickle.load( open(\"cache/train_df.pik\",\"rb\"))\n",
    "# valid_df = pickle.load( open(\"cache/valid_df.pik\",\"rb\"))\n",
    "silent_df = pickle.load(open(\"cache/silent_df.pik\",\"rb\"))\n",
    "# unknown_df = pickle.load(open(\"cache/unknown_df.pik\",\"rb\"))\n",
    "# test_df =  pickle.load(open(\"cache/test_df.pik\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler = AudioSegment.silent(duration=1000, frame_rate = 16000)\n",
    "silence_files_AS = [AudioSegment.from_wav(x) for x in silent_df.wav_file.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_to_1sec(wav):\n",
    "    #fill to 1 second\n",
    "    L = 1000 #16000  # 1 sec\n",
    "    sample_rate = 16000\n",
    "    \n",
    "    if len(wav) > L:\n",
    "        i = np.random.randint(0, len(wav) - L)\n",
    "        wav = wav[i:(i+L)]\n",
    "    elif len(wav) < L:\n",
    "        rem_len = L - len(wav)\n",
    "        wav = AudioSegment.silent(rem_len,frame_rate=sample_rate) + wav\n",
    "        \n",
    "    return wav    \n",
    "\n",
    "def augment_wav(wav,pval=0.5):\n",
    "    sample_rate = 16000\n",
    "    L = 1000 #16000  # 1 sec\n",
    "    \n",
    "    #adjust speed, with 50% chance\n",
    "#     wav = speed_change(wav,1.+random.uniform(-1, 1)*0.05) if np.random.random() < pval else wav\n",
    "    \n",
    "    \n",
    "    #adjust volume\n",
    "    db_adjustment = random.uniform(-1, 1)*10\n",
    "    wav = wav + db_adjustment if np.random.random() < pval else wav\n",
    "     \n",
    "        \n",
    "    #fill to 1 second\n",
    "    wav = fill_to_1sec(wav)        \n",
    "        \n",
    "    #shift the audio by 10 ms\n",
    "    shift_length = 100\n",
    "    if np.random.random() < 0.5: #shift to left\n",
    "        wav = wav[:L-shift_length]+ AudioSegment.silent(shift_length,frame_rate=sample_rate)\n",
    "    else: #shift to right\n",
    "        wav = AudioSegment.silent(shift_length,frame_rate=sample_rate) + wav[shift_length:]\n",
    "        \n",
    "        \n",
    "        \n",
    "    #blend original file with background noise     \n",
    "    if np.random.random() < pval:\n",
    "        noise = random.choice(silence_files_AS)\n",
    "        db_delta = (wav.dBFS - noise.dBFS) -10.\n",
    "\n",
    "        if db_delta< 0: #reduce intensity of loud background; if it's too silent, leave it be\n",
    "            noise = noise  + db_delta\n",
    "        wav = wav.overlay(noise)\n",
    " \n",
    "    return wav\n",
    "\n",
    "\n",
    "def AS_to_raw(as_file):\n",
    "\twav = np.array(as_file.get_array_of_samples().tolist())      \n",
    "#     wav = wav.astype(np.float32) / np.iinfo(np.int16).max\n",
    "\treturn wav   \n",
    "\n",
    "\n",
    "AS_to_raw = numba.jit(AS_to_raw)\n",
    "\n",
    "\n",
    "def process_wav_file(record, reshape=False, augment=True,pval=0.5 ,output_format='logmel'):\n",
    "    \n",
    "    if type(record) == str: # test files\n",
    "        fname = record\n",
    "        label = \"test\"\n",
    "    else:    \n",
    "        fname  = record.wav_file\n",
    "        label = record.label\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    if \"raw_AS_wav\" in record: \n",
    "        wav = record.raw_AS_wav\n",
    "    else:\n",
    "        wav = AudioSegment.from_wav(fname)\n",
    "        \n",
    "        \n",
    "    \n",
    "    if (not label in [\"silence\"]) and augment: #no augmentation for sample files \n",
    "        wav = augment_wav(wav,pval)\n",
    "\n",
    "    else: #make sure segment is 1 second\n",
    "        wav = fill_to_1sec(wav)\n",
    "\n",
    "        \n",
    "    samples = AS_to_raw(wav)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if output_format == \"logmel\":\n",
    "        output = log_mel(samples,reshape=reshape)\n",
    "    else:\n",
    "        output = samples\n",
    "    \n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precomputing augs for faster neural net training\n",
    "def precompute_augmentations(df,num_repeats=4):\n",
    "    \n",
    "    df_aug= pd.concat([df]*num_repeats)\n",
    "    df_aug['raw'] = df_aug.wav_file.apply(process_wav_file)\n",
    "    df = pd.concat([df, df_aug])\n",
    "    \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 15s, sys: 28.8 s, total: 28min 44s\n",
      "Wall time: 14min 16s\n"
     ]
    }
   ],
   "source": [
    "%time unknown_df = precompute_augmentations(unknown_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump( train_df,open(\"cache/train_df_waug.pik\",\"wb\"),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(unknown_df, open(\"cache/unknown_df_waug.pik\",\"wb\"),protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 472 µs\n",
      "CPU times: user 0 ns, sys: 8 ms, total: 8 ms\n",
      "Wall time: 4.14 ms\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.29 ms\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.12 ms\n"
     ]
    }
   ],
   "source": [
    "#profiling: it's too costly to do augmentation on the fly with low gpu utilization; \n",
    "# could be explained by the small gpu-cost vs cpu-preprocessing: gpu batches finish much sooner than cpu is done with augmentation\n",
    "# it's not fully due to reading from disk, but mostly about manipulation the wav file\n",
    "%time w = AudioSegment.from_wav(train_df.wav_file.iloc[0])\n",
    "# %time w = load_audio_file(train_df.wav_file.iloc[0])\n",
    "# wn = np.random.randn(len(w))\n",
    "# %time w = w + 0.005*wn\n",
    "\n",
    "%time ww = train_df.loc[0,\"raw_AS_wav\"]\n",
    "%time ww = AS_to_raw(w)\n",
    "%time ww = AS_to_raw_fast(w)\n",
    "\n",
    "# %time w = augment_wav(w)\n",
    "# %time ww =  AS_to_raw(w)\n",
    "# %time w = w.get_array_of_samples()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# %time train_df['raw_AS_wav'] = train_df.wav_file.apply(lambda fname: AudioSegment.from_wav(fname))\n",
    "\n",
    "# %time valid_df[\"raw_AS_wav\"]  = valid_df.wav_file.apply(lambda fname: AudioSegment.from_wav(fname))\n",
    "\n",
    "# %time silent_df[\"raw_AS_wav\"]  = silent_df.wav_file.apply(lambda fname: AudioSegment.from_wav(fname))\n",
    "\n",
    "# %time unknown_df[\"raw_AS_wav\"]   = unknown_df.wav_file.apply(lambda fname: AudioSegment.from_wav(fname))\n",
    "\n",
    "\n",
    "# pickle.dump( train_df,open(\"cache/train_df.pik\",\"wb\"),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(valid_df, open(\"cache/valid_df.pik\",\"wb\"),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(silent_df, open(\"cache/silent_df.pik\",\"wb\"),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(unknown_df, open(\"cache/unknown_df.pik\",\"wb\"),protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:32.519795Z",
     "start_time": "2017-11-17T09:03:32.483881Z"
    },
    "_cell_guid": "144c6e60-8a83-437d-8b8a-ea065af90923",
    "_uuid": "22e0e6c718171167089fb6df36d3dc43a1029992"
   },
   "outputs": [],
   "source": [
    "#no augmentation since the auto encoder has already seen all the train AND test files \n",
    "\n",
    "def train_generator(train_batch_size):\n",
    "    while True:\n",
    "        \n",
    "        this_train = train_df.groupby('label_id').apply(lambda x: x.sample(n = 2000))\n",
    "        extra_data_size = int(this_train.shape[0]* 0.1)\n",
    "        this_train = pd.concat([silent_df.sample(extra_data_size),\n",
    "                                this_train,\n",
    "                                unknown_df.sample(extra_data_size*4)],axis=0 )\n",
    "        \n",
    "        this_train.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "        shuffled_ids = random.sample(range(this_train.shape[0]), this_train.shape[0])\n",
    "        \n",
    "        for start in range(0, len(shuffled_ids), train_batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            \n",
    "            end = min(start + train_batch_size, len(shuffled_ids))\n",
    "            i_train_batch = shuffled_ids[start:end]\n",
    "            for i in i_train_batch:\n",
    "                x_batch.append(this_train.loc[i,'raw'].T)\n",
    "#                 x_batch.append(process_wav_file(this_train.iloc[i], augment=True).T)\n",
    "\n",
    "                y_batch.append(this_train.label_id.values[i])\n",
    "                \n",
    "            x_batch = 1.- np.array(x_batch)/-80.\n",
    "            y_batch = to_categorical(y_batch, num_classes = len(POSSIBLE_LABELS))\n",
    "            \n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84 ms, sys: 0 ns, total: 84 ms\n",
      "Wall time: 80.2 ms\n"
     ]
    }
   ],
   "source": [
    "%time t = next(train_generator(256))[0][0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:32.624289Z",
     "start_time": "2017-11-17T09:03:32.521828Z"
    },
    "_cell_guid": "59a13393-9bc3-4b27-abe2-9c78b3c32ead",
    "_uuid": "6f9a8fbf6e352b1c77b9c22dddf1c5d69382bd5b"
   },
   "outputs": [],
   "source": [
    "def valid_generator(val_batch_size):\n",
    "    while True:\n",
    "        ids = list(range(valid_df.shape[0]))\n",
    "        for start in range(0, len(ids), val_batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + val_batch_size, len(ids))\n",
    "            i_val_batch = ids[start:end]\n",
    "            for i in i_val_batch:\n",
    "                x_batch.append(valid_df.loc[i,'raw'].T)\n",
    "                y_batch.append(valid_df.label_id.values[i])\n",
    "\n",
    "            x_batch = 1.- np.array(x_batch)/-80.\n",
    "            y_batch = to_categorical(y_batch, num_classes = len(POSSIBLE_LABELS))\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "exp_name = \"aebase_aug_drp3_frozen\"\n",
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=5,\n",
    "                           verbose=1),\n",
    "             \n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=3,\n",
    "                               verbose=1,\n",
    "                               epsilon=0.01,\n",
    "                              min_lr=1e-4),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='weights/{}.hdf5'.format(exp_name),\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='min'),\n",
    "             \n",
    "            TensorBoard(log_dir='./logs/logs_{}'.format(exp_name), histogram_freq=0, batch_size=64, write_graph=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a common practice is to choose a filter size in time which spans 2/3 o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps, input_dim , latent_dim = 32,128, 128\n",
    "\n",
    "input_img = Input(shape=(timesteps, input_dim))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Reshape((timesteps, input_dim,1))(input_img)\n",
    "\n",
    "x = Conv2D(64, (10, 10), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (5, 5), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same',name='latent_rep')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (5, 5), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(64, (10, 10), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "decoded  = Reshape((timesteps, input_dim))(decoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights(\"./weights/starter_ae_wtest_conv_rmse_c6.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in autoencoder.layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 4, 16, 32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.get_layer(\"latent_rep\").output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.3\n",
    "\n",
    "x = Conv2D(64, (1, 1), activation='relu', padding='same')(autoencoder.get_layer(\"latent_rep\").output)\n",
    "\n",
    "x = Conv2D(128, (2, 2), activation='relu', padding='same')(x)\n",
    "x = Dropout(p/2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = MaxPooling2D((2,2),padding='same')(x)\n",
    "\n",
    "x = Conv2D(128, (2, 2), activation='relu', padding='same')(x)\n",
    "x = Dropout(p/2)(x)\n",
    "\n",
    "x = GlobalMaxPool2D()(x)\n",
    "\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "x = Dropout(p)(x)\n",
    "\n",
    "x = Dense(len(POSSIBLE_LABELS), activation = 'softmax', name='targets')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model(inputs=[input_img], outputs = x)\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 32, 128, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 128, 64)       6464      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 64, 32)        51232     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 32, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 32, 32)         9248      \n",
      "_________________________________________________________________\n",
      "latent_rep (MaxPooling2D)    (None, 4, 16, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 16, 64)         2112      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 16, 128)        32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 16, 128)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 16, 128)        512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 2, 8, 128)         65664     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "targets (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 186,188\n",
      "Trainable params: 118,988\n",
      "Non-trainable params: 67,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:33.180074Z",
     "start_time": "2017-11-17T09:03:32.625939Z"
    },
    "_cell_guid": "0e13c01e-5662-4679-9b31-bf9347080ae5",
    "_uuid": "a17b3ea5c15c781260f3473f37dd0d36a932a565"
   },
   "outputs": [],
   "source": [
    "# p = 0.5\n",
    "\n",
    "# x_in = Input(shape = (128,32,1)) #1 channel, 99 time, 161 freqs # S : np.ndarray [shape=(n_mels, t)]\n",
    "\n",
    "# x = BatchNormalization()(x_in)\n",
    "\n",
    "# x = Conv2D(64, (9,10),activation='relu',padding='same')(x)\n",
    "# x = Dropout(p)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling2D((3,2),padding='same')(x)\n",
    "\n",
    "# x = Conv2D(128, (4,5),activation='relu',padding='same')(x)\n",
    "# x = Dropout(p)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling2D((3,2),padding='same')(x)\n",
    "\n",
    "\n",
    "# x = Conv2D(128, (2,2),activation='relu',padding='same')(x)\n",
    "# x = Dropout(p)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = MaxPooling2D(padding='same')(x)\n",
    "\n",
    "\n",
    "\n",
    "# x = GlobalMaxPool2D()(x)\n",
    "\n",
    "# # x = Flatten()(x)\n",
    "# x = Dense(64, activation = 'relu')(x) #\n",
    "# x = Dropout(p)(x)\n",
    "\n",
    "# # x = Dense(64, activation = 'relu')(x)\n",
    "# # x = Dropout(0.3)(x)\n",
    "\n",
    "# x = Dense(len(POSSIBLE_LABELS), activation = 'softmax', name='targets')(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = Model(inputs = x_in, outputs = x)\n",
    "# model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>wav_file</th>\n",
       "      <th>raw</th>\n",
       "      <th>raw_AS_wav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>left</td>\n",
       "      <td>4</td>\n",
       "      <td>cb8f8307</td>\n",
       "      <td>./data/train/audio/left/cb8f8307_nohash_1.wav</td>\n",
       "      <td>[[-80.0, -80.0, -53.5190041125, -45.8180520192...</td>\n",
       "      <td>(((&lt;pydub.audio_segment.AudioSegment object at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>left</td>\n",
       "      <td>4</td>\n",
       "      <td>b7a0754f</td>\n",
       "      <td>./data/train/audio/left/b7a0754f_nohash_2.wav</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -80.0, -80.0, -8...</td>\n",
       "      <td>(((&lt;pydub.audio_segment.AudioSegment object at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>left</td>\n",
       "      <td>4</td>\n",
       "      <td>0132a06d</td>\n",
       "      <td>./data/train/audio/left/0132a06d_nohash_3.wav</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -80.0, -80.0, -8...</td>\n",
       "      <td>(((&lt;pydub.audio_segment.AudioSegment object at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>left</td>\n",
       "      <td>4</td>\n",
       "      <td>f92e49f3</td>\n",
       "      <td>./data/train/audio/left/f92e49f3_nohash_4.wav</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -79.1795468386, -61.342...</td>\n",
       "      <td>(((&lt;pydub.audio_segment.AudioSegment object at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>left</td>\n",
       "      <td>4</td>\n",
       "      <td>88053e92</td>\n",
       "      <td>./data/train/audio/left/88053e92_nohash_1.wav</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -74.5488604542, -75.231...</td>\n",
       "      <td>(((&lt;pydub.audio_segment.AudioSegment object at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  label_id   user_id                                       wav_file  \\\n",
       "0  left         4  cb8f8307  ./data/train/audio/left/cb8f8307_nohash_1.wav   \n",
       "1  left         4  b7a0754f  ./data/train/audio/left/b7a0754f_nohash_2.wav   \n",
       "2  left         4  0132a06d  ./data/train/audio/left/0132a06d_nohash_3.wav   \n",
       "3  left         4  f92e49f3  ./data/train/audio/left/f92e49f3_nohash_4.wav   \n",
       "4  left         4  88053e92  ./data/train/audio/left/88053e92_nohash_1.wav   \n",
       "\n",
       "                                                 raw  \\\n",
       "0  [[-80.0, -80.0, -53.5190041125, -45.8180520192...   \n",
       "1  [[-80.0, -80.0, -80.0, -80.0, -80.0, -80.0, -8...   \n",
       "2  [[-80.0, -80.0, -80.0, -80.0, -80.0, -80.0, -8...   \n",
       "3  [[-80.0, -80.0, -80.0, -79.1795468386, -61.342...   \n",
       "4  [[-80.0, -80.0, -80.0, -74.5488604542, -75.231...   \n",
       "\n",
       "                                          raw_AS_wav  \n",
       "0  (((<pydub.audio_segment.AudioSegment object at...  \n",
       "1  (((<pydub.audio_segment.AudioSegment object at...  \n",
       "2  (((<pydub.audio_segment.AudioSegment object at...  \n",
       "3  (((<pydub.audio_segment.AudioSegment object at...  \n",
       "4  (((<pydub.audio_segment.AudioSegment object at...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 14/100\n",
    "300/300 [==============================] - 214s - loss: 0.7612 - acc: 0.7370 - val_loss: 1.0481 - val_acc: 0.6508\n",
    "\n",
    "\n",
    "\n",
    "Epoch 31/100\n",
    "350/350 [==============================] - 227s - loss: 0.4294 - acc: 0.8518 - val_loss: 0.9436 - val_acc: 0.7179\n",
    "Epoch 32/100\n",
    "\n",
    "\n",
    "\n",
    "Epoch 00058: reducing learning rate to 1.00000006569e-06.\n",
    "329/329 [==============================] - 191s - loss: 0.7292 - acc: 0.7521 - val_loss: 0.7132 - val_acc: 0.8770\n",
    "\n",
    "\n",
    "with ae, p=0.\n",
    "Epoch 28/100\n",
    "329/329 [==============================] - 7s - loss: 0.3312 - acc: 0.8829 - val_loss: 0.4124 - val_acc: 0.8579\n",
    "\n",
    "with p=0.4 and /2 for convs \n",
    "Epoch 28/100\n",
    "329/329 [==============================] - 7s - loss: 0.5534 - acc: 0.8056 - val_loss: 0.5074 - val_acc: 0.8312\n",
    "\n",
    "with p=0.2 and /2\n",
    "Epoch 00035: reducing learning rate to 1.0000000475e-05.\n",
    "329/329 [==============================] - 7s - loss: 0.2414 - acc: 0.9137 - val_loss: 0.3686 - val_acc: 0.8811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395.71875"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape[0]*(1.2)//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:42:31.48233Z",
     "start_time": "2017-11-17T09:03:33.355603Z"
    },
    "_cell_guid": "5f3d1b09-500f-410e-820a-8eaab24b6ebb",
    "_uuid": "528ec66a0a6caca952273ab916e609625839b19e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "989/989 [==============================] - 36s - loss: 0.9326 - acc: 0.6730 - val_loss: 0.7046 - val_acc: 0.7614\n",
      "Epoch 2/100\n",
      "989/989 [==============================] - 36s - loss: 0.8052 - acc: 0.7186 - val_loss: 0.6923 - val_acc: 0.7546\n",
      "Epoch 3/100\n",
      "989/989 [==============================] - 36s - loss: 0.7264 - acc: 0.7456 - val_loss: 0.5195 - val_acc: 0.8225\n",
      "Epoch 4/100\n",
      "989/989 [==============================] - 37s - loss: 0.6787 - acc: 0.7629 - val_loss: 0.5026 - val_acc: 0.8255\n",
      "Epoch 5/100\n",
      "989/989 [==============================] - 37s - loss: 0.6375 - acc: 0.7777 - val_loss: 0.4492 - val_acc: 0.8552\n",
      "Epoch 6/100\n",
      "989/989 [==============================] - 36s - loss: 0.6040 - acc: 0.7901 - val_loss: 0.4668 - val_acc: 0.8383\n",
      "Epoch 7/100\n",
      "989/989 [==============================] - 37s - loss: 0.5798 - acc: 0.7983 - val_loss: 0.4556 - val_acc: 0.8407\n",
      "Epoch 8/100\n",
      "989/989 [==============================] - 37s - loss: 0.5590 - acc: 0.8046 - val_loss: 0.4429 - val_acc: 0.8529\n",
      "Epoch 9/100\n",
      "987/989 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.8127\n",
      "Epoch 00008: reducing learning rate to 0.00010000000475.\n",
      "989/989 [==============================] - 37s - loss: 0.5395 - acc: 0.8128 - val_loss: 0.4673 - val_acc: 0.8437\n",
      "Epoch 10/100\n",
      "989/989 [==============================] - 37s - loss: 0.4668 - acc: 0.8370 - val_loss: 0.3737 - val_acc: 0.8778\n",
      "Epoch 11/100\n",
      "989/989 [==============================] - 37s - loss: 0.4454 - acc: 0.8429 - val_loss: 0.3679 - val_acc: 0.8778\n",
      "Epoch 12/100\n",
      "989/989 [==============================] - 37s - loss: 0.4412 - acc: 0.8455 - val_loss: 0.3767 - val_acc: 0.8721\n",
      "Epoch 13/100\n",
      "989/989 [==============================] - 37s - loss: 0.4282 - acc: 0.8506 - val_loss: 0.3638 - val_acc: 0.8799\n",
      "Epoch 14/100\n",
      "989/989 [==============================] - 37s - loss: 0.4272 - acc: 0.8511 - val_loss: 0.3612 - val_acc: 0.8812\n",
      "Epoch 15/100\n",
      "989/989 [==============================] - 37s - loss: 0.4259 - acc: 0.8518 - val_loss: 0.3673 - val_acc: 0.8755\n",
      "Epoch 16/100\n",
      "989/989 [==============================] - 37s - loss: 0.4217 - acc: 0.8518 - val_loss: 0.3606 - val_acc: 0.8775\n",
      "Epoch 17/100\n",
      "989/989 [==============================] - 37s - loss: 0.4152 - acc: 0.8536 - val_loss: 0.3620 - val_acc: 0.8788\n",
      "Epoch 18/100\n",
      "989/989 [==============================] - 37s - loss: 0.4134 - acc: 0.8543 - val_loss: 0.3661 - val_acc: 0.8778\n",
      "Epoch 19/100\n",
      "989/989 [==============================] - 37s - loss: 0.4131 - acc: 0.8549 - val_loss: 0.3392 - val_acc: 0.8849\n",
      "Epoch 20/100\n",
      "989/989 [==============================] - 37s - loss: 0.4098 - acc: 0.8562 - val_loss: 0.3563 - val_acc: 0.8822\n",
      "Epoch 21/100\n",
      "989/989 [==============================] - 37s - loss: 0.4066 - acc: 0.8570 - val_loss: 0.3596 - val_acc: 0.8802\n",
      "Epoch 22/100\n",
      "989/989 [==============================] - 37s - loss: 0.4049 - acc: 0.8581 - val_loss: 0.3597 - val_acc: 0.8782\n",
      "Epoch 23/100\n",
      "989/989 [==============================] - 37s - loss: 0.4006 - acc: 0.8598 - val_loss: 0.3717 - val_acc: 0.8734\n",
      "Epoch 24/100\n",
      "989/989 [==============================] - 37s - loss: 0.3959 - acc: 0.8601 - val_loss: 0.3574 - val_acc: 0.8829\n",
      "Epoch 25/100\n",
      "989/989 [==============================] - 37s - loss: 0.3955 - acc: 0.8615 - val_loss: 0.3477 - val_acc: 0.8809\n",
      "Epoch 00024: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "history = model.fit_generator(generator=train_generator(batch_size),\n",
    "                              steps_per_epoch=train_df.shape[0]*(1.2)//batch_size,\n",
    "                              epochs=100,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=valid_generator(batch_size),\n",
    "                              validation_steps=int(np.ceil(valid_df.shape[0]/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers:\n",
    "    l.trainable = True\n",
    "    \n",
    "    \n",
    "model.compile(optimizer=Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T10:24:59.198625Z",
     "start_time": "2017-11-17T10:24:59.081762Z"
    },
    "_cell_guid": "0c99ba3b-e8ca-40cb-8d29-2b0e89a385c7",
    "_uuid": "429139ca4f71487c6cfe3e8dfbb6a659eb9bb9c8"
   },
   "outputs": [],
   "source": [
    "model.load_weights('./weights/{}.hdf5'.format(exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"aebase_aug_drp3_finetune\"\n",
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=5,\n",
    "                           verbose=1),\n",
    "             \n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=3,\n",
    "                               verbose=1,\n",
    "                               epsilon=0.01,\n",
    "                              min_lr=1e-6),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='weights/{}.hdf5'.format(exp_name),\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "989/989 [==============================] - 85s - loss: 0.4034 - acc: 0.8585 - val_loss: 0.3539 - val_acc: 0.8835\n",
      "Epoch 2/100\n",
      "989/989 [==============================] - 85s - loss: 0.3916 - acc: 0.8625 - val_loss: 0.3505 - val_acc: 0.8849\n",
      "Epoch 3/100\n",
      "989/989 [==============================] - 85s - loss: 0.3909 - acc: 0.8629 - val_loss: 0.3477 - val_acc: 0.8822\n",
      "Epoch 4/100\n",
      "989/989 [==============================] - 85s - loss: 0.3857 - acc: 0.8651 - val_loss: 0.3477 - val_acc: 0.8819\n",
      "Epoch 5/100\n",
      "989/989 [==============================] - 85s - loss: 0.3845 - acc: 0.8654 - val_loss: 0.3238 - val_acc: 0.8920\n",
      "Epoch 6/100\n",
      "989/989 [==============================] - 85s - loss: 0.3817 - acc: 0.8664 - val_loss: 0.3444 - val_acc: 0.8822\n",
      "Epoch 7/100\n",
      "989/989 [==============================] - 85s - loss: 0.3786 - acc: 0.8676 - val_loss: 0.3361 - val_acc: 0.8883\n",
      "Epoch 8/100\n",
      "989/989 [==============================] - 85s - loss: 0.3751 - acc: 0.8679 - val_loss: 0.3391 - val_acc: 0.8903\n",
      "Epoch 9/100\n",
      "988/989 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8680\n",
      "Epoch 00008: reducing learning rate to 1e-06.\n",
      "989/989 [==============================] - 85s - loss: 0.3742 - acc: 0.8680 - val_loss: 0.3562 - val_acc: 0.8809\n",
      "Epoch 10/100\n",
      "989/989 [==============================] - 85s - loss: 0.3688 - acc: 0.8717 - val_loss: 0.3404 - val_acc: 0.8890\n",
      "Epoch 11/100\n",
      "989/989 [==============================] - 85s - loss: 0.3706 - acc: 0.8700 - val_loss: 0.3368 - val_acc: 0.8873\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=train_generator(batch_size),\n",
    "                              steps_per_epoch=train_df.shape[0]*(1.2)//batch_size,\n",
    "                              epochs=100,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=valid_generator(batch_size),\n",
    "                              validation_steps=int(np.ceil(valid_df.shape[0]/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./weights/{}.hdf5'.format(exp_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### valid evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict_generator(valid_generator(64),steps=int(np.ceil(valid_df.shape[0]/64.)))\n",
    "val_preds = np.argmax(val_preds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3091, 12)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = oe.fit_transform(valid_df.label_id.values.reshape(-1, 1)).todense()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = oe.transform(val_preds.reshape(-1, 1)).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        yes       0.95      0.94      0.95       261\n",
      "         no       0.82      0.85      0.84       270\n",
      "         up       0.89      0.91      0.90       260\n",
      "       down       0.92      0.88      0.90       264\n",
      "       left       0.92      0.90      0.91       247\n",
      "      right       0.97      0.86      0.91       256\n",
      "         on       0.96      0.86      0.91       257\n",
      "        off       0.92      0.88      0.90       256\n",
      "       stop       0.96      0.87      0.91       246\n",
      "         go       0.86      0.76      0.81       260\n",
      "    silence       0.98      0.99      0.98       257\n",
      "    unknown       0.62      0.92      0.74       257\n",
      "\n",
      "avg / total       0.90      0.89      0.89      3091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_true,val_preds,target_names=POSSIBLE_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        yes       0.93      0.95      0.94       261\n",
      "         no       0.84      0.81      0.82       270\n",
      "         up       0.88      0.90      0.89       260\n",
      "       down       0.83      0.91      0.87       264\n",
      "       left       0.92      0.88      0.90       247\n",
      "      right       0.93      0.89      0.91       256\n",
      "         on       0.89      0.89      0.89       257\n",
      "        off       0.93      0.86      0.89       256\n",
      "       stop       0.89      0.89      0.89       246\n",
      "         go       0.81      0.78      0.79       260\n",
      "    silence       1.00      1.00      1.00       257\n",
      "    unknown       0.71      0.78      0.74       257\n",
      "\n",
      "avg / total       0.88      0.88      0.88      3091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_true,val_preds,target_names=POSSIBLE_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T10:28:14.451612Z",
     "start_time": "2017-11-17T10:28:13.307142Z"
    },
    "_cell_guid": "72f27090-c0d1-4d0b-8027-34c915429a79",
    "_uuid": "1007977fccadecdae582ec5d8d52dd3c4c3010aa"
   },
   "outputs": [],
   "source": [
    "test_paths = glob(os.path.join('./data/', 'test/audio/*wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158538"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pickle.load( open(\"cache/test_df.pik\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T10:32:14.882322Z",
     "start_time": "2017-11-17T10:32:14.863617Z"
    },
    "_cell_guid": "c6d9b369-9979-4bcd-8540-4653e6544f84",
    "_uuid": "6a0bb3c22b7b5c43db0ec5673333ab3de8f08724"
   },
   "outputs": [],
   "source": [
    "def test_generator(test_batch_size,augment=False):\n",
    "    while True:\n",
    "        ids = list(range(test_df.shape[0]))\n",
    "        \n",
    "        for start in range(0, len(ids), test_batch_size):\n",
    "            x_batch = []\n",
    "            end = min(start + test_batch_size, len(ids))\n",
    "            i_test_batch = ids[start:end]\n",
    "#             this_paths = test_paths[start:end]\n",
    "#             for x in this_paths:\n",
    "            for i in i_test_batch:\n",
    "            #WATCHOUT > NO AUG\n",
    "#                 x_batch.append(process_wav_file(x).T) #,reshape=False,augment=augment,pval=0.5))\n",
    "                x_batch.append(test_df.loc[i,'raw'].T)\n",
    "\n",
    "            x_batch = np.array(x_batch)\n",
    "            x_batch = 1.- np.array(x_batch)/-80.\n",
    "            \n",
    "            yield x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_generator(val_batch_size):\n",
    "    while True:\n",
    "        ids = list(range(valid_df.shape[0]))\n",
    "        for start in range(0, len(ids), val_batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + val_batch_size, len(ids))\n",
    "            i_val_batch = ids[start:end]\n",
    "            for i in i_val_batch:\n",
    "                x_batch.append(valid_df.loc[i,'raw'].T)\n",
    "                y_batch.append(valid_df.label_id.values[i])\n",
    "\n",
    "            x_batch = 1.- np.array(x_batch)/-80.\n",
    "            y_batch = to_categorical(y_batch, num_classes = len(POSSIBLE_LABELS))\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-17T10:32:45.947Z"
    },
    "_cell_guid": "1fb8aed4-de12-43c5-84bf-b803e3d640fa",
    "_uuid": "631a38cb0013e5772f6987854145ad76ecf6c430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 34s    \n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(test_generator(256,augment=False), int(np.ceil(len(test_paths)/256.)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"cache/predictions_{}.npy\".format(exp_name),predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158538, 12)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pyramid_noaug = np.load('cache/predictions_pyramid_noaug.npy')\n",
    "predictions_model_with_ae_base_drp2_1 = np.load('cache/predictions_model_with_ae_base_drp2_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.mean([predictions,predictions_pyramid_noaug,predictions_model_with_ae_base_drp2_1], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477/2478 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# num_aug = 2 \n",
    "# for i in range(num_aug):\n",
    "#     predictions +=  model.predict_generator(test_generator(64,augment=True), int(np.ceil(len(test_paths)/64.)), verbose=1)\n",
    "    \n",
    "\n",
    "# predictions = predictions/(num_aug + 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T11:30:44.236246Z",
     "start_time": "2017-11-17T11:30:44.21858Z"
    },
    "_cell_guid": "b1cdab5c-9816-4690-87d8-de2c97cf0e7d",
    "_uuid": "24eb7e512eace4567494e0a8e356a826f4283c4d"
   },
   "outputs": [],
   "source": [
    "classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((158538,), 158538)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes.shape, len(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 12769.,   6288.,   7058.,   6322.,   6488.,   7524.,   5727.,\n",
       "          6044.,   7517.,  92801.]),\n",
       " array([  0. ,   1.1,   2.2,   3.3,   4.4,   5.5,   6.6,   7.7,   8.8,\n",
       "          9.9,  11. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDpKkjnGQJHX+F8MQE0BKM3oUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61b65e2a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(classes,bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 12482.,   6260.,   6059.,   6403.,   5591.,   7249.,   6102.,\n",
       "          6070.,   7064.,  95258.]),\n",
       " array([  0. ,   1.1,   2.2,   3.3,   4.4,   5.5,   6.6,   7.7,   8.8,\n",
       "          9.9,  11. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVI3Q0OS1M3QkCR1MzQkSd3+H69CtA3A8hTeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f621de23090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(classes,bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 13909.,   6442.,  10015.,   6507.,   8567.,   8867.,   5738.,\n",
       "          6198.,   9714.,  82581.]),\n",
       " array([  0. ,   1.1,   2.2,   3.3,   4.4,   5.5,   6.6,   7.7,   8.8,\n",
       "          9.9,  11. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fed01ff2e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(classes,bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 14037.,   7124.,  12571.,   6486.,  10360.,  11502.,   6549.,\n",
       "          6624.,  10275.,  73010.]),\n",
       " array([  0. ,   1.1,   2.2,   3.3,   4.4,   5.5,   6.6,   7.7,   8.8,\n",
       "          9.9,  11. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1cad211550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(classes,bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T11:31:11.212517Z",
     "start_time": "2017-11-17T11:31:10.786357Z"
    },
    "_cell_guid": "1da523cf-fdbf-4ab1-9300-0147155aa247",
    "_uuid": "f25d4e626202aa115bd0460f4de8d07f9727c83e"
   },
   "outputs": [],
   "source": [
    "### last batch will contain padding, so remove duplicates\n",
    "submission = dict()\n",
    "for i in range(len(test_paths)):\n",
    "    fname, label = os.path.basename(test_paths[i]), id2name[classes[i]]\n",
    "    submission[fname] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T11:32:05.154527Z",
     "start_time": "2017-11-17T11:32:04.983371Z"
    },
    "_cell_guid": "9a95d147-3f4b-4386-8597-5fa60be43542",
    "_uuid": "bdf63bce43a0525a02ac18ca3f90aeba06ce6e99"
   },
   "outputs": [],
   "source": [
    "with open('subm/submission_{}.csv'.format(exp_name), 'w') as fout:\n",
    "    fout.write('fname,label\\n')\n",
    "    for fname, label in submission.items():\n",
    "        fout.write('{},{}\\n'.format(fname, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "_cell_guid": "8bea6850-15c6-44e7-bdb4-9555ad196f85",
    "_uuid": "555315ef622793711ff5643928dac874c8cb0ed2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='subm/submission_aebase_aug_drp3_finetune_blend.csv' target='_blank'>subm/submission_aebase_aug_drp3_finetune_blend.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/keyword_spotting/subm/submission_aebase_aug_drp3_finetune_blend.csv"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.lib.display import FileLink\n",
    "\n",
    "FileLink('subm/submission_{}.csv'.format(exp_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
