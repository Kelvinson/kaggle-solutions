{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:29.196238Z",
     "start_time": "2017-11-17T09:03:28.644004Z"
    },
    "_cell_guid": "679e0d3e-646d-4e96-9eb0-b362d8c6e51f",
    "_uuid": "0d05e5ce89af3e25d1c1fb244d021a1cfa1a058c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import array \n",
    "\n",
    "from pydub import AudioSegment\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Conv2D, Flatten, GlobalMaxPooling1D, MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPool2D, concatenate, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Input, GRU, RepeatVector, BatchNormalization, TimeDistributed, Conv1D\n",
    "from keras.layers import GlobalAveragePooling1D, LSTM, MaxPooling1D, CuDNNLSTM, Bidirectional\n",
    "from keras import backend as K\n",
    "from keras.layers import  Conv2D, MaxPooling2D, UpSampling2D, Lambda, Reshape\n",
    "import keras\n",
    "from keras.layers import AveragePooling1D\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import numba\n",
    "from keras import losses\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "from keras.layers import Embedding, Flatten, Input, merge\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [x[0].split('/')[-1] for x in os.walk(\"data/train/audio/\")]\n",
    " \n",
    "\n",
    "\n",
    "exclusions = [\"\",\"_background_noise_\"]\n",
    "POSSIBLE_LABELS = [item for item in all_labels if item not in exclusions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:29.210749Z",
     "start_time": "2017-11-17T09:03:29.19832Z"
    },
    "_cell_guid": "8ab00801-08b9-44d3-a063-32e82dbf8f58",
    "_uuid": "53c19941676690454dd4b91109976b6c59cb7a40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POSSIBLE_LABELS = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "id2name = {i: name for i, name in enumerate(POSSIBLE_LABELS)}\n",
    "name2id = {name: i for i, name in id2name.items()}\n",
    "len(id2name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create validation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pickle.load( open(\"cache/train_df_all_labels.pik\",\"rb\"))\n",
    "valid_df = pickle.load( open(\"cache/valid_df_all_labels.pik\",\"rb\"))\n",
    "silent_df = pickle.load(open(\"cache/silent_df.pik\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"label_user\"] = train_df[\"label\"]+train_df[\"user_id\"]\n",
    "valid_df[\"label_user\"] = valid_df[\"label\"]+valid_df[\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(train_batch_size):\n",
    "    \n",
    "    \n",
    "    #choose records which have same speaker repeating them more than once \n",
    "    tmp = train_df.groupby(\"label_user\").filter(lambda x: len(x) > 1) \n",
    "    tmp.reset_index(inplace=True,drop=True)\n",
    "    while True:\n",
    "        \n",
    "        #choose a random label, utterances should be from same label\n",
    "        this_train = tmp[tmp.label_id==np.random.randint(0,len(id2name))] \n",
    "    \n",
    "        #group by distinct speakers for this label\n",
    "        g = this_train.groupby(\"label_user\")\n",
    "        all_keys = list(g.groups.keys())\n",
    "        \n",
    "        if not all_keys:\n",
    "            continue      \n",
    "        \n",
    "        for _ in range(0, 2): #almost take from every pair in this label\n",
    "            \n",
    "            x_batch_A_1 = []\n",
    "            x_batch_A_2 = []\n",
    "            x_batch_B = []\n",
    "        \n",
    "       \n",
    "            \n",
    "            for i in range(train_batch_size):\n",
    "                \n",
    "                #choose a random speaker A\n",
    "                i = np.random.choice(range(len(all_keys)))\n",
    "                similars = g.get_group(all_keys[i])\n",
    "                \n",
    "                #choose 2 distinct utterances from speaker A \n",
    "                sim_1, sim_2 = np.random.choice(range(len(similars)),2,replace=False)\n",
    "                A_1 , A_2 = similars.raw.iloc[sim_1], similars.raw.iloc[sim_2] \n",
    "                \n",
    "                # choose a different speaker B\n",
    "                possible_diffs = range(len(all_keys)-1) if (i == len(all_keys)-1) else range(len(all_keys))\n",
    "                diff_i = np.random.choice(possible_diffs) \n",
    "                diff_i = diff_i+1 if diff_i >=i else diff_i\n",
    "                diff_i = min(diff_i, len(all_keys)-1)\n",
    "                diffs = g.get_group(all_keys[diff_i])\n",
    "                \n",
    "                #choose a random utternace from B's records\n",
    "                B = diffs.raw.iloc[np.random.choice(range(len(diffs)))]\n",
    "                \n",
    "                \n",
    "                # create triplet\n",
    "                x_batch_A_1.append(A_1.T)\n",
    "                x_batch_A_2.append(A_2.T)\n",
    "                x_batch_B.append(B.T)\n",
    "#                 x_batch.append([A_1.T,A_2.T,B.T])\n",
    "                \n",
    "            x_batch_A_1 = 1.- np.array(x_batch_A_1)/-80.\n",
    "            x_batch_A_2 = 1.- np.array(x_batch_A_2)/-80.\n",
    "            x_batch_B = 1.- np.array(x_batch_B)/-80.\n",
    "        \n",
    "            y_batch = np.ones((train_batch_size)) \n",
    "            \n",
    "            yield [x_batch_A_1,x_batch_A_2,x_batch_B], y_batch# [y_batch,y_batch,y_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.31655370011274542, 0.31404827090262433, 0.37890711528615456)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = train_generator(32)\n",
    "\n",
    "tt = next(t)[0]\n",
    "\n",
    "tt[0].mean(), tt[1].mean(), tt[2].mean(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 256)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tt[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_generator(train_batch_size):\n",
    "    \n",
    "    \n",
    "    #choose records which have same speaker repeating them more than once \n",
    "    tmp = valid_df.groupby(\"label_user\").filter(lambda x: len(x) > 1) \n",
    "    tmp.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        #choose a random label, utterances should be from same label\n",
    "        this_train = tmp[tmp.label_id==np.random.randint(0,len(id2name))] \n",
    "    \n",
    "        #group by distinct speakers for this label\n",
    "        g = this_train.groupby(\"label_user\")\n",
    "        all_keys = list(g.groups.keys())\n",
    "        \n",
    "        if not all_keys:\n",
    "            continue \n",
    "            \n",
    "            \n",
    "        for _ in range(0, 2): #for _ in range(0, len(all_keys)): #almost take from every pair in this label\n",
    "            \n",
    "            x_batch_A_1 = []\n",
    "            x_batch_A_2 = []\n",
    "            x_batch_B = []\n",
    "            \n",
    "            for i in range(train_batch_size):\n",
    "                \n",
    "                #choose a random speaker A\n",
    "                i = np.random.choice(range(len(all_keys)))\n",
    "                similars = g.get_group(all_keys[i])\n",
    "                \n",
    "                #choose 2 distinct utterances from speaker A \n",
    "                sim_1, sim_2 = np.random.choice(range(len(similars)),2,replace=False)\n",
    "                A_1 , A_2 = similars.raw.iloc[sim_1], similars.raw.iloc[sim_2] \n",
    "                \n",
    "                # choose a different speaker B\n",
    "                possible_diffs = range(len(all_keys)-1) if (i == len(all_keys)-1) else range(len(all_keys))\n",
    "                diff_i = np.random.choice(possible_diffs) \n",
    "                diff_i = diff_i+1 if diff_i >=i else diff_i\n",
    "                diff_i = min(diff_i, len(all_keys)-1)\n",
    "                diffs = g.get_group(all_keys[diff_i])\n",
    "                \n",
    "                #choose a random utternace from B's records\n",
    "                B = diffs.raw.iloc[np.random.choice(range(len(diffs)))]\n",
    "                \n",
    "                \n",
    "                # create triplet\n",
    "                x_batch_A_1.append(A_1.T)\n",
    "                x_batch_A_2.append(A_2.T)\n",
    "                x_batch_B.append(B.T)\n",
    "#                 x_batch.append([A_1.T,A_2.T,B.T])\n",
    "                \n",
    "            x_batch_A_1 = 1.- np.array(x_batch_A_1)/-80.\n",
    "            x_batch_A_2 = 1.- np.array(x_batch_A_2)/-80.\n",
    "            x_batch_B = 1.- np.array(x_batch_B)/-80.\n",
    "        \n",
    "            y_batch = np.ones((train_batch_size)) \n",
    "            \n",
    "            yield x_batch_A_1,x_batch_A_2,x_batch_B, y_batch #[y_batch,y_batch,y_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = valid_generator(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 512 ms, sys: 0 ns, total: 512 ms\n",
      "Wall time: 506 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 256)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time next(v)[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1,X_2,X_3, y_valid = [], [],[], []\n",
    "\n",
    "for i in range( 400):\n",
    "    x1,x2,x3, y_v = next(v)\n",
    "    X_1.append(x1) \n",
    "    X_2.append(x2) \n",
    "    X_3.append(x3) \n",
    "    \n",
    "    y_valid.append(y_v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.vstack(X_1)\n",
    "X_2 = np.vstack(X_2)\n",
    "X_3 = np.vstack(X_3)\n",
    "y_valid = np.hstack(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25600, 32, 256), (25600,))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_relu(x):\n",
    "    x = BatchNormalization()(x)    \n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps, input_dim , latent_dim = 32,256, 128\n",
    "p = .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def triplet_loss(y_true, y_pred):\n",
    "#     mse1 = losses.mean_squared_error(y_pred[0],y_pred[1]) #K.sqrt(y_pred[0] - y_pred[1]).mean(axis=-1)\n",
    "#     mse2 = losses.mean_squared_error(y_pred[0],y_pred[2])#K.sqr(y_pred[0] - y_pred[2]).mean(axis=-1)\n",
    "    \n",
    "#     return K.maximum(0., mse1 - mse2 + 1) - y_true[0]*0\n",
    "\n",
    "\n",
    "# def triplet_loss( y_true, y_pred):\n",
    "\n",
    "#     embeddings = K.reshape(y_pred, (-1, 3, latent_dim))\n",
    "\n",
    "#     positive_distance = K.mean(K.square(embeddings[:,0] - embeddings[:,1]),axis=-1)\n",
    "#     negative_distance = K.mean(K.square(embeddings[:,0] - embeddings[:,2]),axis=-1)\n",
    "#     return K.mean(K.maximum(0.0, positive_distance - negative_distance + 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(y_true, y_pred):\n",
    "\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "\n",
    "def bpr_triplet_loss(X):\n",
    "\n",
    "    positive_item_latent, negative_item_latent, user_latent = X\n",
    "\n",
    "    # BPR loss\n",
    "    loss = 1.0 - K.sigmoid(\n",
    "        K.sum(user_latent * positive_item_latent, axis=-1, keepdims=True) -\n",
    "        K.sum(user_latent * negative_item_latent, axis=-1, keepdims=True))\n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embedder(filter_size=2):\n",
    "    v_input = Input(shape=( timesteps,input_dim))\n",
    "    x = BatchNormalization()(v_input)\n",
    "    \n",
    "    x =  Reshape((timesteps, input_dim,1))(x)\n",
    "    \n",
    "#     x = Conv1D(64,filter_size,padding='same')(x)\n",
    "#     x = batch_relu(x)\n",
    "#     x = Conv1D(64,filter_size,padding='same')(x)\n",
    "#     x = batch_relu(x)\n",
    "    \n",
    "    \n",
    "#     x  = MaxPooling1D(2)(x) \n",
    "    \n",
    "#     x = Conv1D(128,filter_size,padding='same')(x)\n",
    "#     x = batch_relu(x)\n",
    "#     x = Conv1D(128,filter_size,padding='same')(x)\n",
    "#     x = batch_relu(x)\n",
    "\n",
    "#     x  = AveragePooling1D(2)(x)   \n",
    "    \n",
    "# #     x = Bidirectional(CuDNNLSTM(128,return_sequences=False))(x)\n",
    "# #     x = BatchNormalization()(x)\n",
    "# #     x = Dense(128,activation='relu')(x)\n",
    "    \n",
    "#     x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(32,(7,7),padding='same')(x) #was 32\n",
    "    x = batch_relu(x)\n",
    "\n",
    "    x = Conv2D(64,(3,3),padding='same')(x)\n",
    "    x = batch_relu(x)\n",
    "\n",
    "    \n",
    "    x = MaxPooling2D((1,3))(x)\n",
    "    \n",
    "    x = Conv2D(64,(3,3),padding='same')(x)\n",
    "    x = batch_relu(x)\n",
    "    x = Conv2D(64,(3,3),padding='same')(x)\n",
    "    x = batch_relu(x)\n",
    "\n",
    "    \n",
    "    \n",
    "    x = MaxPooling2D((2,3))(x)\n",
    "    \n",
    "\n",
    "    x = Conv2D(128,(3,3),padding='same')(x)\n",
    "    x = batch_relu(x)\n",
    "    x = Conv2D(128,(3,3),padding='same')(x)\n",
    "    x = batch_relu(x)\n",
    "\n",
    "    x = MaxPooling2D((1,3))(x)\n",
    "    \n",
    "    \n",
    "    x = Conv2D(128,(3,3),padding='same')(x)\n",
    "    x = batch_relu(x)\n",
    "    x = Conv2D(128,(3,3),padding='same')(x)\n",
    "    x = batch_relu(x)\n",
    "    print int(x.shape[-1]) * int(x.shape[-2])\n",
    "    x = Reshape((16,9*128))(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNLSTM(128,return_sequences=False))(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    return Model(v_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:23: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "xA1 = Input(shape=(timesteps, input_dim)) #1 channel, 99 time, 161 freqs # S : np.ndarray [shape=(n_mels, t)]\n",
    "xA2 = Input(shape=(timesteps, input_dim)) #1 channel, 99 time, 161 freqs # S : np.ndarray [shape=(n_mels, t)]\n",
    "xB = Input(shape=(timesteps, input_dim)) #1 channel, 99 time, 161 freqs # S : np.ndarray [shape=(n_mels, t)]\n",
    "\n",
    "# xA1_freq = Reshape((input_dim, timesteps))(xA1)\n",
    "# xA2_freq = Reshape((input_dim, timesteps))(xA2)\n",
    "# xB_freq = Reshape((input_dim, timesteps))(xB)\n",
    "\n",
    "\n",
    "\n",
    "embedding = embedder(5)\n",
    "\n",
    "xA1_freq = embedding(xA1)\n",
    "xA2_freq = embedding(xA2)\n",
    "xB_freq = embedding(xB)\n",
    "\n",
    "loss = merge(\n",
    "    [xA1_freq, xB_freq, xA2_freq],\n",
    "    mode=bpr_triplet_loss,\n",
    "    name='loss',\n",
    "    output_shape=(1,))\n",
    "\n",
    "model = Model(\n",
    "        inputs=[xA1, xA2, xB],\n",
    "        outputs=loss) #concatenate([xA1_freq,xA2_freq,xB_freq],axis=0)\n",
    "\n",
    "model.compile(Adam(lr=1e-3),identity_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_40 (InputLayer)        (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 32, 256)           1024      \n",
      "_________________________________________________________________\n",
      "reshape_22 (Reshape)         (None, 32, 256, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 256, 32)       1600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 32, 256, 32)       128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 32, 256, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 32, 256, 64)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 32, 256, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 32, 256, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 85, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 32, 85, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 32, 85, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 32, 85, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 85, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 32, 85, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 32, 85, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 16, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 16, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 9, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 9, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 16, 9, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 16, 9, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 9, 128)        147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 16, 9, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16, 9, 128)        0         \n",
      "_________________________________________________________________\n",
      "reshape_23 (Reshape)         (None, 16, 1152)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               1312768   \n",
      "=================================================================\n",
      "Total params: 1,927,296\n",
      "Trainable params: 1,925,312\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"speaker_embedding_fixedval_conv2dlstm\" \n",
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=5,\n",
    "                           verbose=1),\n",
    "             \n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=3,\n",
    "                               verbose=1,\n",
    "                               epsilon=0.01,\n",
    "                              min_lr=1e-5),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='weights/{}.hdf5'.format(exp_name),\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='min')            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           (None, 32, 256)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_39 (InputLayer)           (None, 32, 256)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_38 (InputLayer)           (None, 32, 256)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_12 (Model)                (None, 256)          1927296     input_37[0][0]                   \n",
      "                                                                 input_38[0][0]                   \n",
      "                                                                 input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "loss (Merge)                    (None, 1)            0           model_12[1][0]                   \n",
      "                                                                 model_12[3][0]                   \n",
      "                                                                 model_12[2][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,927,296\n",
      "Trainable params: 1,925,312\n",
      "Non-trainable params: 1,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freqconv\n",
    "\n",
    "Epoch 15/100\n",
    "905/905 [==============================] - 94s 104ms/step - loss: 0.0247 - acc: 0.0245 - val_loss: 0.0230 - val_acc: 0.0236\n",
    "\n",
    "\n",
    "timeconv wlstm\n",
    "Epoch 17/100\n",
    "905/905 [==============================] - 84s 92ms/step - loss: 0.1163 - val_loss: 0.1131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"weights/speaker_embedding_timeconv_wlstm_c2.hdf5\")\n",
    "model.load_weights(\"weights/{}.hdf5\".format(exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "905/905 [==============================] - 881s 974ms/step - loss: 0.0754 - val_loss: 0.0697\n",
      "Epoch 2/100\n",
      "905/905 [==============================] - 871s 962ms/step - loss: 0.0431 - val_loss: 0.0634\n",
      "Epoch 3/100\n",
      "905/905 [==============================] - 870s 962ms/step - loss: 0.0351 - val_loss: 0.0490\n",
      "Epoch 4/100\n",
      "905/905 [==============================] - 870s 962ms/step - loss: 0.0324 - val_loss: 0.0454\n",
      "Epoch 5/100\n",
      "905/905 [==============================] - 872s 963ms/step - loss: 0.0286 - val_loss: 0.0364\n",
      "Epoch 6/100\n",
      "905/905 [==============================] - 871s 963ms/step - loss: 0.0266 - val_loss: 0.0357\n",
      "Epoch 7/100\n",
      "905/905 [==============================] - 872s 964ms/step - loss: 0.0247 - val_loss: 0.0329\n",
      "Epoch 8/100\n",
      "905/905 [==============================] - 870s 961ms/step - loss: 0.0234 - val_loss: 0.0368\n",
      "Epoch 9/100\n",
      "904/905 [============================>.] - ETA: 0s - loss: 0.0205\n",
      "Epoch 00009: reducing learning rate to 0.00010000000475.\n",
      "905/905 [==============================] - 872s 963ms/step - loss: 0.0206 - val_loss: 0.0288\n",
      "Epoch 10/100\n",
      "905/905 [==============================] - 871s 962ms/step - loss: 0.0181 - val_loss: 0.0248\n",
      "Epoch 11/100\n",
      "905/905 [==============================] - 874s 965ms/step - loss: 0.0157 - val_loss: 0.0241\n",
      "Epoch 12/100\n",
      "905/905 [==============================] - 871s 962ms/step - loss: 0.0139 - val_loss: 0.0212\n",
      "Epoch 13/100\n",
      "905/905 [==============================] - 874s 966ms/step - loss: 0.0140 - val_loss: 0.0227\n",
      "Epoch 14/100\n",
      "904/905 [============================>.] - ETA: 0s - loss: 0.0133\n",
      "Epoch 00014: reducing learning rate to 1.0000000475e-05.\n",
      "905/905 [==============================] - 873s 965ms/step - loss: 0.0133 - val_loss: 0.0209\n",
      "Epoch 15/100\n",
      "905/905 [==============================] - 868s 959ms/step - loss: 0.0124 - val_loss: 0.0210\n",
      "Epoch 16/100\n",
      "905/905 [==============================] - 867s 958ms/step - loss: 0.0122 - val_loss: 0.0207\n",
      "Epoch 17/100\n",
      "905/905 [==============================] - 867s 958ms/step - loss: 0.0122 - val_loss: 0.0206\n",
      "Epoch 18/100\n",
      "905/905 [==============================] - 867s 958ms/step - loss: 0.0120 - val_loss: 0.0194\n",
      "Epoch 19/100\n",
      "905/905 [==============================] - 866s 957ms/step - loss: 0.0125 - val_loss: 0.0193\n",
      "Epoch 20/100\n",
      "905/905 [==============================] - 865s 955ms/step - loss: 0.0122 - val_loss: 0.0200\n",
      "Epoch 21/100\n",
      "905/905 [==============================] - 867s 958ms/step - loss: 0.0122 - val_loss: 0.0191\n",
      "Epoch 22/100\n",
      "905/905 [==============================] - 866s 957ms/step - loss: 0.0128 - val_loss: 0.0206\n",
      "Epoch 23/100\n",
      "905/905 [==============================] - 867s 958ms/step - loss: 0.0119 - val_loss: 0.0198\n",
      "Epoch 24/100\n",
      "905/905 [==============================] - 869s 960ms/step - loss: 0.0128 - val_loss: 0.0189\n",
      "Epoch 25/100\n",
      "905/905 [==============================] - 870s 961ms/step - loss: 0.0127 - val_loss: 0.0197\n",
      "Epoch 26/100\n",
      "905/905 [==============================] - 875s 967ms/step - loss: 0.0112 - val_loss: 0.0219\n",
      "Epoch 27/100\n",
      "905/905 [==============================] - 871s 962ms/step - loss: 0.0118 - val_loss: 0.0187\n",
      "Epoch 28/100\n",
      "905/905 [==============================] - 869s 960ms/step - loss: 0.0118 - val_loss: 0.0196\n",
      "Epoch 29/100\n",
      "905/905 [==============================] - 867s 958ms/step - loss: 0.0114 - val_loss: 0.0209\n",
      "Epoch 30/100\n",
      "678/905 [=====================>........] - ETA: 3:10 - loss: 0.0115"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-b78d8d85029a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#                               validation_data=valid_generator(batch_size),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#                               validation_steps=int(np.ceil(valid_df.shape[0]//batch_size)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                               validation_data=([X_1,X_2,X_3],y_valid))#\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2145\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2146\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2147\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "history = model.fit_generator(generator=train_generator(batch_size),\n",
    "                              steps_per_epoch=train_df.shape[0]//batch_size,\n",
    "                              epochs=100,\n",
    "                              callbacks=callbacks,\n",
    "#                               validation_data=valid_generator(batch_size),\n",
    "#                               validation_steps=int(np.ceil(valid_df.shape[0]//batch_size)))\n",
    "                              validation_data=([X_1,X_2,X_3],y_valid))#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights/{}.hdf5\".format(exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.save_weights(\"weights/speaker_embedding_conv2dlstm.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996727\n",
      "0.00337073\n",
      "0.990294\n",
      "-0.000155765\n",
      "0.991019\n",
      "0.00466726\n",
      "1.00052\n",
      "-0.00549822\n",
      "0.990437\n",
      "-0.00571059\n",
      "0.984998\n",
      "-0.00494674\n",
      "0.982686\n",
      "-0.00640985\n",
      "0.989239\n",
      "-0.000872991\n",
      "1.0419\n",
      "0.00901419\n"
     ]
    }
   ],
   "source": [
    "for l in embedding.layers:\n",
    "    try:\n",
    "        print np.mean(l.get_weights()[0])\n",
    "    except:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821782\n",
      "0.00257948\n",
      "0.97807\n",
      "-0.0011363\n",
      "0.990386\n",
      "0.00306028\n",
      "0.995768\n",
      "0.00208017\n",
      "1.00084\n",
      "-0.00353018\n",
      "1.10434\n",
      "0.0176663\n"
     ]
    }
   ],
   "source": [
    "for l in embedding.layers:\n",
    "    try:\n",
    "        print np.mean(l.get_weights()[0])\n",
    "    except:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82218\n",
      "0.00224655\n",
      "0.978326\n",
      "-0.000955906\n",
      "0.990576\n",
      "0.00329765\n",
      "0.99577\n",
      "0.00218155\n",
      "1.00076\n",
      "-0.00361477\n",
      "1.10443\n",
      "0.0177416\n"
     ]
    }
   ],
   "source": [
    "for l in embedding.layers:\n",
    "    try:\n",
    "        print np.mean(l.get_weights()[0])\n",
    "    except:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-0.000116561\n",
      "1.0\n",
      "-0.000564231\n",
      "1.0\n",
      "0.00077817\n",
      "1.0\n",
      "-6.0897e-05\n",
      "1.0\n",
      "0.000275884\n",
      "1.0\n",
      "-0.000213044\n"
     ]
    }
   ],
   "source": [
    "for l in embedding.layers:\n",
    "    try:\n",
    "        print np.mean(l.get_weights()[0])\n",
    "    except:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
